{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4ViA8Bbs0T3",
        "outputId": "41c78af9-50a3-49e5-d974-b37559bdfe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xai-bench'...\n",
            "remote: Enumerating objects: 367, done.\u001b[K\n",
            "remote: Counting objects: 100% (367/367), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 367 (delta 172), reused 331 (delta 153), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (367/367), 5.48 MiB | 28.32 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/syzymon/xai-bench"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r xai-bench/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o3LkhFggtewN",
        "outputId": "26af9e0a-6c94-490c-d88d-26e3a45a73f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting seaborn==0.11.1\n",
            "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.0 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting dill==0.3.3\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.2.1\n",
            "  Downloading matplotlib-3.2.1-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting pandas==1.0.5\n",
            "  Downloading pandas-1.0.5-cp38-cp38-manylinux1_x86_64.whl (10.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0 MB 25.8 MB/s \n",
            "\u001b[?25hCollecting commentjson==0.9.0\n",
            "  Downloading commentjson-0.9.0.tar.gz (8.7 kB)\n",
            "Collecting tqdm==4.46.0\n",
            "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.9 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting shap==0.41.0\n",
            "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
            "\u001b[K     |████████████████████████████████| 575 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting lime==0.2.0.1\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.1->-r xai-bench/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.1->-r xai-bench/requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.1->-r xai-bench/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.2.1->-r xai-bench/requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas==1.0.5->-r xai-bench/requirements.txt (line 6)) (2022.6)\n",
            "Collecting lark-parser<0.8.0,>=0.7.1\n",
            "  Downloading lark-parser-0.7.8.tar.gz (276 kB)\n",
            "\u001b[K     |████████████████████████████████| 276 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==0.24.2->-r xai-bench/requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==0.24.2->-r xai-bench/requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.8/dist-packages (from shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (21.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (1.5.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.8/dist-packages (from lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (0.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.1->-r xai-bench/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (2022.10.10)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (2.8.8)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r xai-bench/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->shap==0.41.0->-r xai-bench/requirements.txt (line 10)) (3.11.0)\n",
            "Building wheels for collected packages: commentjson, lime, lark-parser\n",
            "  Building wheel for commentjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for commentjson: filename=commentjson-0.9.0-py3-none-any.whl size=12092 sha256=b34b7da175a946fc080ed52932e96b7df0bcb500b377e6d0a2a16dacde4915b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/1c/b5/6f1b1411615716f6d2b52b9301bfaf032ed5f68d4c7d547be8\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=05b8357455d098f899f20e4c0a864cb79403783c6ca4469d7a84d14ad34ce61c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/a6/20/cc1e293fcdb67ede666fed293cb895395e7ecceb4467779546\n",
            "  Building wheel for lark-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lark-parser: filename=lark_parser-0.7.8-py2.py3-none-any.whl size=62526 sha256=162a1dd70f3969d389fb8bb5000c34c48d84d195a6949bd3fac3509055767e16\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/b5/2b/b6896f25d9b272b4f72db3a45a15cb0b7a6e43d7980c936a15\n",
            "Successfully built commentjson lime lark-parser\n",
            "Installing collected packages: numpy, scipy, matplotlib, tqdm, slicer, scikit-learn, pandas, lark-parser, shap, seaborn, lime, dill, commentjson\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.11.2\n",
            "    Uninstalling seaborn-0.11.2:\n",
            "      Successfully uninstalled seaborn-0.11.2\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.5 which is incompatible.\n",
            "xarray-einstats 0.3.0 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.5 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.46.0 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.5 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 1.0.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed commentjson-0.9.0 dill-0.3.3 lark-parser-0.7.8 lime-0.2.0.1 matplotlib-3.2.1 numpy-1.19.5 pandas-1.0.5 scikit-learn-0.24.2 scipy-1.4.1 seaborn-0.11.1 shap-0.41.0 slicer-0.0.7 tqdm-4.46.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd xai-bench/ && python main_driver.py --mode regression --seed 7 --experiment --experiment-json configs/experiment_config.jsonc --no-logs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kos3B-LgtzmC",
        "outputId": "e29318f2-5d5b-42e7-d820-41a4873e0877"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-08 17:13:39 INFO     \n",
            " Dataset config is: {\"name\": \"gaussianPiecewiseConstant\", \"data_kwargs\": {\"mu\": \"np.zeros(5)\", \"dim\": 5, \"rho\": 0.5, \"weight\": \"np.array([4, 3, 2, 1, 0])\", \"noise\": 0.01, \"num_train_samples\": 100, \"num_val_samples\": 100}}\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "100% 100/100 [00:30<00:00,  3.24it/s]\n",
            "2022-12-08 17:14:10 INFO     Explaining dtree with random\n",
            "2022-12-08 17:14:16 INFO     Explaining dtree with shap\n",
            "2022-12-08 17:14:26 INFO     Explaining dtree with shapr\n",
            "100it [00:34,  2.91it/s]\n",
            "100it [00:29,  3.34it/s]\n",
            "2022-12-08 17:15:36 INFO     Explaining dtree with brutekernelshap\n",
            "100it [00:01, 72.07it/s]\n",
            "100it [00:01, 75.11it/s]\n",
            "2022-12-08 17:15:44 INFO     Explaining dtree with maple\n",
            "100% 100/100 [00:01<00:00, 70.01it/s]\n",
            "100% 100/100 [00:01<00:00, 69.94it/s]\n",
            "invalid value encountered in true_divide\n",
            "invalid value encountered in true_divide\n",
            "2022-12-08 17:15:53 INFO     Explaining dtree with lime\n",
            "100% 100/100 [00:31<00:00,  3.18it/s]\n",
            "100% 100/100 [00:31<00:00,  3.15it/s]\n",
            "100% 100/100 [01:31<00:00,  1.09it/s]\n",
            "2022-12-08 17:18:34 INFO     Explaining mlp with random\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:18:45 INFO     Explaining mlp with shap\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:18:59 INFO     Explaining mlp with shapr\n",
            "100it [00:35,  2.79it/s]\n",
            "100it [00:40,  2.44it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:20:26 INFO     Explaining mlp with brutekernelshap\n",
            "100it [00:02, 46.58it/s]\n",
            "100it [00:02, 48.02it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:20:41 INFO     Explaining mlp with maple\n",
            "100% 100/100 [00:01<00:00, 68.68it/s]\n",
            "100% 100/100 [00:01<00:00, 69.42it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:20:56 INFO     Explaining mlp with lime\n",
            "100% 100/100 [00:32<00:00,  3.11it/s]\n",
            "100% 100/100 [00:32<00:00,  3.10it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 17:22:11 INFO     \n",
            "Experiment results : {\n",
            "    \"dataset\": \"gaussianPiecewiseConstant\",\n",
            "    \"dataset_kwargs\": {\n",
            "        \"mu\": \"np.zeros(5)\",\n",
            "        \"dim\": 5,\n",
            "        \"rho\": 0.5,\n",
            "        \"weight\": \"np.array([4, 3, 2, 1, 0])\",\n",
            "        \"noise\": 0.01,\n",
            "        \"num_train_samples\": 100,\n",
            "        \"num_val_samples\": 100\n",
            "    },\n",
            "    \"models\": {\n",
            "        \"dtree\": {\n",
            "            \"random\": {\n",
            "                \"roar_faithfulness\": 0.08440614487763347,\n",
            "                \"roar_monotonicity\": 0.4775,\n",
            "                \"faithfulness\": 0.07965175700927421,\n",
            "                \"monotonicity\": 0.4275,\n",
            "                \"shapley\": 1.189994594467615,\n",
            "                \"shapley_corr\": -0.054945629752779626,\n",
            "                \"infidelity\": 0.3921929865347131\n",
            "            },\n",
            "            \"shap\": {\n",
            "                \"roar_faithfulness\": 0.329422238637459,\n",
            "                \"roar_monotonicity\": 0.54,\n",
            "                \"faithfulness\": 0.8079658190107353,\n",
            "                \"monotonicity\": 0.46,\n",
            "                \"shapley\": 0.029511131326118985,\n",
            "                \"shapley_corr\": 0.8513978368572488,\n",
            "                \"infidelity\": 0.12319982238228543\n",
            "            },\n",
            "            \"shapr\": {\n",
            "                \"roar_faithfulness\": 0.35868256688604466,\n",
            "                \"roar_monotonicity\": 0.5025,\n",
            "                \"faithfulness\": 0.7926341466195003,\n",
            "                \"monotonicity\": 0.44,\n",
            "                \"shapley\": 0.010476187471402464,\n",
            "                \"shapley_corr\": 0.917301303245245,\n",
            "                \"infidelity\": 0.09061448771408082\n",
            "            },\n",
            "            \"brutekernelshap\": {\n",
            "                \"roar_faithfulness\": 0.3596287930020371,\n",
            "                \"roar_monotonicity\": 0.515,\n",
            "                \"faithfulness\": 0.7972291195016693,\n",
            "                \"monotonicity\": 0.4575,\n",
            "                \"shapley\": 0.029511131335289667,\n",
            "                \"shapley_corr\": 0.8513978368693099,\n",
            "                \"infidelity\": 0.12358448086134598\n",
            "            },\n",
            "            \"maple\": {\n",
            "                \"roar_faithfulness\": -0.14117240258359798,\n",
            "                \"roar_monotonicity\": 0.545,\n",
            "                \"faithfulness\": 0.5488035672778405,\n",
            "                \"monotonicity\": 0.375,\n",
            "                \"shapley\": 0.09942659012774684,\n",
            "                \"shapley_corr\": 0.5914481707639667,\n",
            "                \"infidelity\": 0.1449559047950712\n",
            "            },\n",
            "            \"lime\": {\n",
            "                \"roar_faithfulness\": 0.3685925861777913,\n",
            "                \"roar_monotonicity\": 0.485,\n",
            "                \"faithfulness\": 0.7188611250037961,\n",
            "                \"monotonicity\": 0.455,\n",
            "                \"shapley\": 0.0865795207632742,\n",
            "                \"shapley_corr\": 0.723175039216394,\n",
            "                \"infidelity\": 0.11125022615572266\n",
            "            }\n",
            "        },\n",
            "        \"mlp\": {\n",
            "            \"random\": {\n",
            "                \"roar_faithfulness\": 0.026063901040228377,\n",
            "                \"roar_monotonicity\": 0.4175,\n",
            "                \"faithfulness\": 0.058237686426612145,\n",
            "                \"monotonicity\": 0.43,\n",
            "                \"shapley\": 0.9915995576401838,\n",
            "                \"shapley_corr\": -0.0447354021408411,\n",
            "                \"infidelity\": 0.08165271535850828\n",
            "            },\n",
            "            \"shap\": {\n",
            "                \"roar_faithfulness\": 0.30354915052787873,\n",
            "                \"roar_monotonicity\": 0.5125,\n",
            "                \"faithfulness\": 0.7881831279958743,\n",
            "                \"monotonicity\": 0.4775,\n",
            "                \"shapley\": 0.029364975356415128,\n",
            "                \"shapley_corr\": 0.7894711000954229,\n",
            "                \"infidelity\": 0.007461135902131077\n",
            "            },\n",
            "            \"shapr\": {\n",
            "                \"roar_faithfulness\": 0.33226541796985387,\n",
            "                \"roar_monotonicity\": 0.5225,\n",
            "                \"faithfulness\": 0.8211025028335548,\n",
            "                \"monotonicity\": 0.4475,\n",
            "                \"shapley\": 0.009107752484125264,\n",
            "                \"shapley_corr\": 0.9097715268033382,\n",
            "                \"infidelity\": 0.0070691834138478194\n",
            "            },\n",
            "            \"brutekernelshap\": {\n",
            "                \"roar_faithfulness\": 0.3750100987075691,\n",
            "                \"roar_monotonicity\": 0.495,\n",
            "                \"faithfulness\": 0.774386170242889,\n",
            "                \"monotonicity\": 0.475,\n",
            "                \"shapley\": 0.029364968385751502,\n",
            "                \"shapley_corr\": 0.789471100100508,\n",
            "                \"infidelity\": 0.007473021222438989\n",
            "            },\n",
            "            \"maple\": {\n",
            "                \"roar_faithfulness\": 0.43104357593837916,\n",
            "                \"roar_monotonicity\": 0.48,\n",
            "                \"faithfulness\": 0.7855566244174834,\n",
            "                \"monotonicity\": 0.4575,\n",
            "                \"shapley\": 0.03242779636757026,\n",
            "                \"shapley_corr\": 0.7832636570798015,\n",
            "                \"infidelity\": 0.010517878180945673\n",
            "            },\n",
            "            \"lime\": {\n",
            "                \"roar_faithfulness\": 0.41111812280899784,\n",
            "                \"roar_monotonicity\": 0.5225,\n",
            "                \"faithfulness\": 0.7419127473062317,\n",
            "                \"monotonicity\": 0.4375,\n",
            "                \"shapley\": 0.0680929758546039,\n",
            "                \"shapley_corr\": 0.7333384763729401,\n",
            "                \"infidelity\": 0.01407824575766485\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"model_perfs\": {\n",
            "        \"dtree\": [\n",
            "            \"0.00\",\n",
            "            \"0.24\"\n",
            "        ],\n",
            "        \"mlp\": [\n",
            "            \"0.18\",\n",
            "            \"0.25\"\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd xai-bench/ && python main_driver.py --mode regression --seed 7 --experiment --experiment-json configs/experiment_config.jsonc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WuUTIitxdmA",
        "outputId": "4f907052-f6c2-409d-9bca-56d6178b2952"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-08 19:44:12 INFO     \n",
            " Dataset config is: {\"name\": \"gaussianPiecewiseConstant\", \"data_kwargs\": {\"mu\": \"np.zeros(5)\", \"dim\": 5, \"rho\": 0.5, \"weight\": \"np.array([4, 3, 2, 1, 0])\", \"noise\": 0.01, \"num_train_samples\": 1000, \"num_val_samples\": 100}}\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "100% 100/100 [00:35<00:00,  2.81it/s]\n",
            "2022-12-08 19:44:48 INFO     Explaining dtree with random\n",
            "2022-12-08 19:44:56 INFO     Explaining dtree with shap\n",
            "invalid value encountered in true_divide\n",
            "invalid value encountered in true_divide\n",
            "2022-12-08 19:45:13 INFO     Explaining dtree with shapr\n",
            "100it [04:09,  2.50s/it]\n",
            "1000it [41:56,  2.52s/it]\n",
            "2022-12-08 20:31:27 INFO     Explaining dtree with brutekernelshap\n",
            "100it [00:01, 52.94it/s]\n",
            "1000it [00:18, 53.22it/s]\n",
            "2022-12-08 20:31:56 INFO     Explaining dtree with maple\n",
            "100% 100/100 [00:01<00:00, 67.55it/s]\n",
            "100% 1000/1000 [00:15<00:00, 66.05it/s]\n",
            "2022-12-08 20:32:23 INFO     Explaining dtree with lime\n",
            "100% 100/100 [00:33<00:00,  2.96it/s]\n",
            "100% 1000/1000 [05:21<00:00,  3.11it/s]\n",
            "100% 100/100 [01:33<00:00,  1.07it/s]\n",
            "2022-12-08 20:40:00 INFO     Explaining mlp with random\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 20:40:22 INFO     Explaining mlp with shap\n",
            "Exact explainer: 1001it [00:11, 84.67it/s]             \n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 20:40:58 INFO     Explaining mlp with shapr\n",
            "100it [05:10,  3.10s/it]\n",
            "1000it [52:38,  3.16s/it]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 21:39:13 INFO     Explaining mlp with brutekernelshap\n",
            "100it [00:06, 16.66it/s]\n",
            "1000it [00:58, 17.02it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 21:40:41 INFO     Explaining mlp with maple\n",
            "100% 100/100 [00:01<00:00, 64.13it/s]\n",
            "100% 1000/1000 [00:16<00:00, 61.12it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 21:41:27 INFO     Explaining mlp with lime\n",
            "100% 100/100 [00:35<00:00,  2.84it/s]\n",
            "100% 1000/1000 [05:34<00:00,  2.99it/s]\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "2022-12-08 21:48:03 INFO     \n",
            "Experiment results : {\n",
            "    \"dataset\": \"gaussianPiecewiseConstant\",\n",
            "    \"dataset_kwargs\": {\n",
            "        \"mu\": \"np.zeros(5)\",\n",
            "        \"dim\": 5,\n",
            "        \"rho\": 0.5,\n",
            "        \"weight\": \"np.array([4, 3, 2, 1, 0])\",\n",
            "        \"noise\": 0.01,\n",
            "        \"num_train_samples\": 1000,\n",
            "        \"num_val_samples\": 100\n",
            "    },\n",
            "    \"models\": {\n",
            "        \"dtree\": {\n",
            "            \"random\": {\n",
            "                \"roar_faithfulness\": 0.02192511876931648,\n",
            "                \"roar_monotonicity\": 0.4475,\n",
            "                \"faithfulness\": 0.018141987847795433,\n",
            "                \"monotonicity\": 0.45,\n",
            "                \"shapley\": 1.0971841614629934,\n",
            "                \"shapley_corr\": 0.13144879560179323,\n",
            "                \"infidelity\": 0.32554776853865697\n",
            "            },\n",
            "            \"shap\": {\n",
            "                \"roar_faithfulness\": 0.2758779917279708,\n",
            "                \"roar_monotonicity\": 0.475,\n",
            "                \"faithfulness\": 0.7964519804776935,\n",
            "                \"monotonicity\": 0.45,\n",
            "                \"shapley\": 0.031070750322514495,\n",
            "                \"shapley_corr\": 0.8675353543868448,\n",
            "                \"infidelity\": 0.13336150096248608\n",
            "            },\n",
            "            \"shapr\": {\n",
            "                \"roar_faithfulness\": 0.3532214721584739,\n",
            "                \"roar_monotonicity\": 0.4925,\n",
            "                \"faithfulness\": 0.8282902834371044,\n",
            "                \"monotonicity\": 0.44,\n",
            "                \"shapley\": 0.009324755328979266,\n",
            "                \"shapley_corr\": 0.9508109316337743,\n",
            "                \"infidelity\": 0.13252792420135268\n",
            "            },\n",
            "            \"brutekernelshap\": {\n",
            "                \"roar_faithfulness\": 0.2941953085889296,\n",
            "                \"roar_monotonicity\": 0.4525,\n",
            "                \"faithfulness\": 0.8171231014789098,\n",
            "                \"monotonicity\": 0.435,\n",
            "                \"shapley\": 0.02878831525441673,\n",
            "                \"shapley_corr\": 0.8799117793013788,\n",
            "                \"infidelity\": 0.13538472618733335\n",
            "            },\n",
            "            \"maple\": {\n",
            "                \"roar_faithfulness\": -0.34059404620693273,\n",
            "                \"roar_monotonicity\": 0.4875,\n",
            "                \"faithfulness\": 0.3988979907949121,\n",
            "                \"monotonicity\": 0.35,\n",
            "                \"shapley\": 0.11919970800730142,\n",
            "                \"shapley_corr\": 0.5227755104274489,\n",
            "                \"infidelity\": 0.1964192453650339\n",
            "            },\n",
            "            \"lime\": {\n",
            "                \"roar_faithfulness\": 0.3999391789803448,\n",
            "                \"roar_monotonicity\": 0.465,\n",
            "                \"faithfulness\": 0.7065073487305971,\n",
            "                \"monotonicity\": 0.42,\n",
            "                \"shapley\": 0.07982379162769038,\n",
            "                \"shapley_corr\": 0.7673683262492702,\n",
            "                \"infidelity\": 0.15020589555215214\n",
            "            }\n",
            "        },\n",
            "        \"mlp\": {\n",
            "            \"random\": {\n",
            "                \"roar_faithfulness\": -0.013772905234916935,\n",
            "                \"roar_monotonicity\": 0.475,\n",
            "                \"faithfulness\": 0.052228719828652785,\n",
            "                \"monotonicity\": 0.42,\n",
            "                \"shapley\": 0.9763907728222095,\n",
            "                \"shapley_corr\": 0.1093846674226058,\n",
            "                \"infidelity\": 0.18892154603311886\n",
            "            },\n",
            "            \"shap\": {\n",
            "                \"roar_faithfulness\": 0.4897576803309713,\n",
            "                \"roar_monotonicity\": 0.5225,\n",
            "                \"faithfulness\": 0.826589879844877,\n",
            "                \"monotonicity\": 0.4425,\n",
            "                \"shapley\": 0.03416059279554351,\n",
            "                \"shapley_corr\": 0.7988814848378647,\n",
            "                \"infidelity\": 0.02933690242535036\n",
            "            },\n",
            "            \"shapr\": {\n",
            "                \"roar_faithfulness\": 0.4890099407143885,\n",
            "                \"roar_monotonicity\": 0.5275,\n",
            "                \"faithfulness\": 0.8680192278488428,\n",
            "                \"monotonicity\": 0.4375,\n",
            "                \"shapley\": 0.00940086191944279,\n",
            "                \"shapley_corr\": 0.9128310000718456,\n",
            "                \"infidelity\": 0.023793299939500287\n",
            "            },\n",
            "            \"brutekernelshap\": {\n",
            "                \"roar_faithfulness\": 0.49681849560734664,\n",
            "                \"roar_monotonicity\": 0.535,\n",
            "                \"faithfulness\": 0.8247878679740652,\n",
            "                \"monotonicity\": 0.4525,\n",
            "                \"shapley\": 0.032053086131162194,\n",
            "                \"shapley_corr\": 0.8191974806130575,\n",
            "                \"infidelity\": 0.0259910243668801\n",
            "            },\n",
            "            \"maple\": {\n",
            "                \"roar_faithfulness\": 0.33328465699186066,\n",
            "                \"roar_monotonicity\": 0.5275,\n",
            "                \"faithfulness\": 0.7875297744324348,\n",
            "                \"monotonicity\": 0.43,\n",
            "                \"shapley\": 0.051782603143979034,\n",
            "                \"shapley_corr\": 0.7821185383628165,\n",
            "                \"infidelity\": 0.031531663759600835\n",
            "            },\n",
            "            \"lime\": {\n",
            "                \"roar_faithfulness\": 0.4964451435184805,\n",
            "                \"roar_monotonicity\": 0.505,\n",
            "                \"faithfulness\": 0.7990881598613676,\n",
            "                \"monotonicity\": 0.465,\n",
            "                \"shapley\": 0.07064425435753208,\n",
            "                \"shapley_corr\": 0.7914544479834703,\n",
            "                \"infidelity\": 0.040979542134277055\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"model_perfs\": {\n",
            "        \"dtree\": [\n",
            "            \"0.00\",\n",
            "            \"0.06\"\n",
            "        ],\n",
            "        \"mlp\": [\n",
            "            \"0.14\",\n",
            "            \"0.22\"\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "2022-12-08 21:48:03 INFO     Saving results in results/logs//gaussianPiecewiseConstant_0.5.log\n",
            "2022-12-08 21:48:03 INFO     Saving results in results/logs//csv/gaussianPiecewiseConstant_0.5.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "def results_to_table(results_path):\n",
        "    with open(results_path, 'r') as f:\n",
        "        res = json.load(f)\n",
        "    for model_name, model_metrics in res['models'].items():\n",
        "        rows = []\n",
        "        index = []\n",
        "        for explainer_name, explainer_metrics in model_metrics.items():\n",
        "            rows.append(explainer_metrics)\n",
        "            index.append(explainer_name)\n",
        "        df = pd.DataFrame(rows, index=index)\n",
        "        display(df)\n",
        "\n",
        "results_to_table('/content/xai-bench/results/logs/gaussianPiecewiseConstant_0.5.log')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "l_wjySl1UwqH",
        "outputId": "67fbceee-636b-42bf-f74d-199882d1f420"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 roar_faithfulness  roar_monotonicity  faithfulness  \\\n",
              "random                        0.02               0.45          0.02   \n",
              "shap                          0.28               0.47          0.80   \n",
              "shapr                         0.35               0.49          0.83   \n",
              "brutekernelshap               0.29               0.45          0.82   \n",
              "maple                        -0.34               0.49          0.40   \n",
              "lime                          0.40               0.47          0.71   \n",
              "\n",
              "                 monotonicity   shapley  shapley_corr  infidelity  \n",
              "random                   0.45  1.10e+00          0.13        0.33  \n",
              "shap                     0.45  3.11e-02          0.87        0.13  \n",
              "shapr                    0.44  9.32e-03          0.95        0.13  \n",
              "brutekernelshap          0.43  2.88e-02          0.88        0.14  \n",
              "maple                    0.35  1.19e-01          0.52        0.20  \n",
              "lime                     0.42  7.98e-02          0.77        0.15  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d54b570-48b3-4bce-a178-54bd89d81d68\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>roar_faithfulness</th>\n",
              "      <th>roar_monotonicity</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>monotonicity</th>\n",
              "      <th>shapley</th>\n",
              "      <th>shapley_corr</th>\n",
              "      <th>infidelity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>random</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.45</td>\n",
              "      <td>1.10e+00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shap</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3.11e-02</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shapr</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.44</td>\n",
              "      <td>9.32e-03</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brutekernelshap</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.43</td>\n",
              "      <td>2.88e-02</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maple</th>\n",
              "      <td>-0.34</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.19e-01</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lime</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.42</td>\n",
              "      <td>7.98e-02</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d54b570-48b3-4bce-a178-54bd89d81d68')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d54b570-48b3-4bce-a178-54bd89d81d68 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d54b570-48b3-4bce-a178-54bd89d81d68');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 roar_faithfulness  roar_monotonicity  faithfulness  \\\n",
              "random                       -0.01               0.47          0.05   \n",
              "shap                          0.49               0.52          0.83   \n",
              "shapr                         0.49               0.53          0.87   \n",
              "brutekernelshap               0.50               0.54          0.82   \n",
              "maple                         0.33               0.53          0.79   \n",
              "lime                          0.50               0.51          0.80   \n",
              "\n",
              "                 monotonicity   shapley  shapley_corr  infidelity  \n",
              "random                   0.42  9.76e-01          0.11        0.19  \n",
              "shap                     0.44  3.42e-02          0.80        0.03  \n",
              "shapr                    0.44  9.40e-03          0.91        0.02  \n",
              "brutekernelshap          0.45  3.21e-02          0.82        0.03  \n",
              "maple                    0.43  5.18e-02          0.78        0.03  \n",
              "lime                     0.47  7.06e-02          0.79        0.04  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b9bf7ef-1a9a-4261-9158-9277ede0c0cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>roar_faithfulness</th>\n",
              "      <th>roar_monotonicity</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>monotonicity</th>\n",
              "      <th>shapley</th>\n",
              "      <th>shapley_corr</th>\n",
              "      <th>infidelity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>random</th>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.42</td>\n",
              "      <td>9.76e-01</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shap</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.44</td>\n",
              "      <td>3.42e-02</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shapr</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.44</td>\n",
              "      <td>9.40e-03</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brutekernelshap</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3.21e-02</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maple</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.43</td>\n",
              "      <td>5.18e-02</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lime</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.47</td>\n",
              "      <td>7.06e-02</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b9bf7ef-1a9a-4261-9158-9277ede0c0cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b9bf7ef-1a9a-4261-9158-9277ede0c0cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b9bf7ef-1a9a-4261-9158-9277ede0c0cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r logs.zip /content/xai-bench/results/logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RViTNHXt7Xz",
        "outputId": "bdab349e-4436-4f15-9153-49b5366feb03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/xai-bench/results/logs/ (stored 0%)\n",
            "  adding: content/xai-bench/results/logs/gaussianPiecewiseConstant_0.5.log (deflated 77%)\n",
            "  adding: content/xai-bench/results/logs/csv/ (stored 0%)\n",
            "  adding: content/xai-bench/results/logs/csv/gaussianPiecewiseConstant_0.5.csv (deflated 54%)\n",
            "  adding: content/xai-bench/results/logs/checkpoints/ (stored 0%)\n",
            "  adding: content/xai-bench/results/logs/checkpoints/gaussianPiecewiseConstant_None.pkl (deflated 23%)\n"
          ]
        }
      ]
    }
  ]
}
