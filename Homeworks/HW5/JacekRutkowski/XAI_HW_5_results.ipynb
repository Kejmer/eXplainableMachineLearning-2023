{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Permutation-based Variable Importance for XGBoost It turns out that the variable age is the most important in terms of permutational variable importance. That is, when we permute the ages of people in the data frame, the new loss obtained with these variables changes a lot.\n",
    "\n",
    "![](plots/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I plotted three most important variables\n",
    "\n",
    "![](plots/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 I trained also Decision Tree Regressor with two sets of hyperparameters and one Random Forest Classifier. Below I present PVI plots for new three models. As it was the case for XGBoost, the 'age' variable is the most important but it obtained lower scores. It can be the case that the loss for XGBoost is smaller which would give bigger denominator in the expression for computation of PVI. It might also be the case that the loss with permuted age is bigger which means that XGBoost relies very much on the age variable and cannot cope without it.\n",
    "\n",
    "![](./plots/p3.png)\n",
    "![](./plots/p4.png)\n",
    "![](./plots/p5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a) Gini-based Variable Importance for XGBoost Classifier\n",
    "\n",
    "![](./plots/p6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b) SHAP variable importance based on the TreeSHAP algorithm\n",
    "\n",
    "![](./plots/p7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 a) In Gini-based Variable Importance for XGBoost Classifier the age variable is the most important one but the differences are not so huge as for PVI, feature importances are much more balanced. In Gini impurity we need to take a single decision tree, calculate the amount that each feature split point improves the performance and then weight the score by the number of observations the node is responsible for. Then these scores are averaged across all the trees used by the model. Thus, lower score for age indicates either that age was relatively rarely used for splitting or that it does not improve the model performance. The results show that rather the first alternative is true and that also other variables were quite often used for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 b) Here also age got a little bit smaller score while other scores remained at the same very low level. In SHAP we want to explain why is the particular score different from the average prediction of the model and which variable has the biggest contribution to this difference. SHAP value may be a little bit lower because when someone had brain stroke and is relatively young, then the age should not contribute to the final prediction. On the other hand, when somebody is old and did not have brain stroke, then the age variable may be then less important than heart disease, hypertension etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('xai_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 (default, Jan  8 2020, 19:59:22) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "af6808631b646a2ea9f8520b5c8cf5ced62fc365bea1d03b49f21ebd9b9b925b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
